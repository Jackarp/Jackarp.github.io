---
title:  "한국어임베딩(1장)"
excerpt: "서론"
header:
  teaser: /assets/images/nlp/KoEmbedding.jpg

categories:
  - NLP
tags:
  - 임베딩
last_modified_at: 2020-07-07T11:30
---

전이학습: 임베딩을 다른 딥러닝 모델의 입력값으로 쓰는 기법  
전이학습(transfer learning) = <span style="color:blue">Pre-training + Fine-tuning</span>       
<span style="color:red">품질좋은 임베딩</span>이 고성능 언어처리 모델을 만드는데 중요구성 요소이다.  


|	<center>단어수준의 임베딩</center>	|	<center>문장수준의 임베딩</center>	|
| :------------------------------------	| :------------------------------------	| 
| 1. NPLM				| 1. LSA				|
| 2. Word2Vec				| 2. Doc2Vec				|
| 3. FastText				| 3. LDA(잠재디리클레할당)			|
| 4. LSA(잠재의미분석)			| 4. ELMO				|
| 5. Glove				| 5. GPT				|
| 6. Swivel				| 6. BERT				|

--- 
임베딩 구축  

|	<center>임베딩 구축 전</center>	|	<center>임베딩 구축 후</center>	|
| :------------------------------------	| :------------------------------------	| 
| 1. 말뭉치 전처리(pre-process) 오픈소스	| 2. 임베딩 파인 튜닝(fine-tuning)		|
| 1) KoNLPy				| 1)					|
| 2) soynlp				| 2)					|
| 3) 구글 센텐스피스(sentencepiece)		| 3)					|


단어 수준 임베딩 기법의 단점 -> 동음이의어의 분간 어려움  -> 문장 수준의 임베딩 기법 주목    

